{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"m26bvOt3k5v5"},"source":["\n","\n","---\n","\n","\n","1. Preprocess all the Data we have in DonorsChoose <a href='https://drive.google.com/drive/folders/1MIwK7BQMev8f5CbDDVNLPaFGB32pFN60'>Dataset</a> use train.csv\n","2. Combine 4 essay's into one column named - 'preprocessed_essays'. \n","3. After step 2 you have to train 3 types of models as discussed below. \n","4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a href='https://datascience.stackexchange.com/a/20192'>this</a> for using auc as a metric \n","5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n","6. You can use any one of the optimizers and choice of Learning rate and momentum, resources: <a href='http://cs231n.github.io/neural-networks-3/'>cs231n class notes</a>, <a href='https://www.youtube.com/watch?v=hd_KFJ5ktUc'>cs231n class video</a>. \n","7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in .ipynb notebook and PDF. \n","8. Use Categorical Cross Entropy as Loss to minimize.\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"-Ov_z3pQk5wD"},"source":["### Model-1\n","\n","Build and Train deep neural network as shown below"]},{"cell_type":"markdown","metadata":{"id":"33oo_ncxk5wE"},"source":["<img src='https://i.imgur.com/w395Yk9.png'>\n","ref: https://i.imgur.com/w395Yk9.png"]},{"cell_type":"markdown","metadata":{"id":"3L1SrW3Ek5wF"},"source":["- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n","- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n","- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"ccL_2m27k5wI"},"source":["- For LSTM, you can choose your sequence padding methods on your own or you can train your LSTM without padding, there is no restriction on that."]},{"cell_type":"markdown","metadata":{"id":"ORydzR_dk5wJ"},"source":["Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GyvPLr5k5wU"},"outputs":[],"source":["# # https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n","# input_layer = Input(shape=(n,))\n","# embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n","# flatten = Flatten()(embedding)"]},{"cell_type":"markdown","metadata":{"id":"WERvYcuHk5wW"},"source":["### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n","### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "]},{"cell_type":"markdown","metadata":{"id":"z0Itgwadk5wY"},"source":["### Model-2"]},{"cell_type":"markdown","metadata":{"id":"HzP3wIQAk5wZ"},"source":["Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "]},{"cell_type":"markdown","metadata":{"id":"XuxMrIE8k5wa"},"source":["<pre>\n","1. Train the TF-IDF on the Train data <br>\n","2. Get the idf value for each word we have in the train data. <br>\n","3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)<br>\n","4. Train the LSTM after removing the Low and High idf value words. (In model-1 Train on total data but in Model-2 train on data after removing some words based on IDF values)\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"etp63gVCk5wc"},"source":["### Model-3"]},{"cell_type":"markdown","metadata":{"id":"jiy_SNklk5wd"},"source":["<img src='https://i.imgur.com/fkQ8nGo.png'>\n","ref: https://i.imgur.com/fkQ8nGo.png"]},{"cell_type":"markdown","metadata":{"id":"h5tChvM8k5wm"},"source":["\n","- __input_seq_total_text_data__: <br>\n","<pre>\n","    . Use text column('essay'), and use the Embedding layer to get word vectors. <br>\n","    . Use given predefined glove word vectors, don't train any word vectors. <br>\n","    . Use LSTM that is given above, get the LSTM output and Flatten that output. <br>\n","    . You are free to preprocess the input text as you needed. <br>\n","</pre>\n","- __Other_than_text_data__:<br>\n","<pre>\n","    . Convert all your Categorical values to onehot coded and then concatenate all these onehot vectors <br>\n","    . Neumerical values and use <a href='https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions'>CNN1D</a> as shown in above figure. <br>\n","    . You are free to choose all CNN parameters like kernel sizes, stride.<br>\n","    \n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"7jMIp4jS9ufl"},"source":["#Starting with the Assignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1MOe7fF9uAS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Activation, Dropout, concatenate\n","from tensorflow.keras.models import Model\n","import keras\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scjsSZw4CeCb"},"outputs":[],"source":["# Mounting google drive\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1911,"status":"ok","timestamp":1620293160694,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"bziJSEyLCmWd","outputId":"3094166b-264d-4e42-d348-6546a918f789"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9XnLxdcBLAR"},"outputs":[],"source":["# Getting preprocessed reviews\n","Data = pd.read_csv(\"/content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/preprocessed_data.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":666},"executionInfo":{"elapsed":3194,"status":"ok","timestamp":1620293162005,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"VKtZt94qDdJ9","outputId":"e8067a5e-8a84-4d4f-f046-170d5d6cc53e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(109248, 9)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>school_state</th>\n","      <th>teacher_prefix</th>\n","      <th>project_grade_category</th>\n","      <th>teacher_number_of_previously_posted_projects</th>\n","      <th>project_is_approved</th>\n","      <th>clean_categories</th>\n","      <th>clean_subcategories</th>\n","      <th>essay</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ca</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>math_science</td>\n","      <td>appliedsciences health_lifescience</td>\n","      <td>i fortunate enough use fairy tale stem kits cl...</td>\n","      <td>725.05</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ut</td>\n","      <td>ms</td>\n","      <td>grades_3_5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>specialneeds</td>\n","      <td>specialneeds</td>\n","      <td>imagine 8 9 years old you third grade classroo...</td>\n","      <td>213.03</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ca</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>literacy_language</td>\n","      <td>literacy</td>\n","      <td>having class 24 students comes diverse learner...</td>\n","      <td>329.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ga</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>appliedlearning</td>\n","      <td>earlydevelopment</td>\n","      <td>i recently read article giving students choice...</td>\n","      <td>481.04</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>wa</td>\n","      <td>mrs</td>\n","      <td>grades_3_5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>literacy_language</td>\n","      <td>literacy</td>\n","      <td>my students crave challenge eat obstacles brea...</td>\n","      <td>17.74</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  school_state  ...   price\n","0           ca  ...  725.05\n","1           ut  ...  213.03\n","2           ca  ...  329.00\n","3           ga  ...  481.04\n","4           wa  ...   17.74\n","\n","[5 rows x 9 columns]"]},"execution_count":18,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["print(Data.shape)\n","Data.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoNhRRc9br0q"},"outputs":[],"source":["Data[\"remaining_numerical_features\"] = Data[\"teacher_number_of_previously_posted_projects\"]+ Data[\"price\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"elapsed":3171,"status":"ok","timestamp":1620293162011,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"aSQSkqQJb4wF","outputId":"d294b7ce-de4e-484b-92e9-a59bdbc2ccf7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>school_state</th>\n","      <th>teacher_prefix</th>\n","      <th>project_grade_category</th>\n","      <th>teacher_number_of_previously_posted_projects</th>\n","      <th>project_is_approved</th>\n","      <th>clean_categories</th>\n","      <th>clean_subcategories</th>\n","      <th>essay</th>\n","      <th>price</th>\n","      <th>remaining_numerical_features</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ca</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>math_science</td>\n","      <td>appliedsciences health_lifescience</td>\n","      <td>i fortunate enough use fairy tale stem kits cl...</td>\n","      <td>725.05</td>\n","      <td>778.05</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ut</td>\n","      <td>ms</td>\n","      <td>grades_3_5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>specialneeds</td>\n","      <td>specialneeds</td>\n","      <td>imagine 8 9 years old you third grade classroo...</td>\n","      <td>213.03</td>\n","      <td>217.03</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  school_state teacher_prefix  ...   price  remaining_numerical_features\n","0           ca            mrs  ...  725.05                        778.05\n","1           ut             ms  ...  213.03                        217.03\n","\n","[2 rows x 10 columns]"]},"execution_count":20,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["Data.head(2)"]},{"cell_type":"markdown","metadata":{"id":"3BD1dq7yb_ty"},"source":["Splitting the Data First into Train and text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zq6grLPfb_V9"},"outputs":[],"source":["Y = Data[\"project_is_approved\"]\n","Data.drop(labels=\"project_is_approved\",axis=1,inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8K1YG0qWddF2"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(Data, Y, test_size=0.25,stratify=Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":908,"status":"ok","timestamp":1620287173850,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"7c-V9bzKfYhj","outputId":"1fd5b48a-7bc6-4d29-a092-bd3b666314d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(81936, 9)\n","(27312, 9)\n","(81936,)\n","(27312,)\n"]}],"source":["print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azJLEpn5yTBs"},"outputs":[],"source":["#https://stackoverflow.com/questions/49984905/count-number-of-words-per-row\n","#https://stackoverflow.com/questions/42943291/what-does-keras-io-preprocessing-sequence-pad-sequences-do\n","\n","# max_length= max([len(x.split()) for x in X_train[\"essay\"].tolist()])\n","# print(max_length)\n","# Since the Max length of the reviews is 339. we'll take max length as 300\n","#Few reviews will be truncated this way but it should be okay\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JWZLUeXTuRh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oRFs_LjFjDm-"},"source":["Featurzing text data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53980,"status":"ok","timestamp":1619594892230,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"oBrHuJEwjC5b","outputId":"3a6f6508-b316-461e-f11e-30382879bb78"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[  14 2629  770 ...    0    0    0]\n"," [   4  216  462 ...    0    0    0]\n"," [  29 3450  219 ...    0    0    0]\n"," ...\n"," [   2 4546   72 ...    0    0    0]\n"," [   4    1   78 ...    0    0    0]\n"," [   4    1  105 ...    0    0    0]]\n"]}],"source":["#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n","\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","t = Tokenizer()\n","\n","t.fit_on_texts(X_train[\"essay\"])\n","\n","vocab_size = len(t.word_index) + 1\n","\n","# integer encode the documents\n","encoded_docs = t.texts_to_sequences(X_train[\"essay\"])\n","#print(encoded_docs)\n","# pad documents to a max length of max_length words\n","padded_docs = pad_sequences(encoded_docs, maxlen=300, padding='post')\n","print(padded_docs)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GAg_TOahCe9"},"outputs":[],"source":["encoded_docs = t.texts_to_sequences(X_test[\"essay\"])\n","padded_docs_test = pad_sequences(encoded_docs, maxlen=300, padding='pre')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSbqbPd2IC9Y"},"outputs":[],"source":["import pickle\n","\n","glove_vector = open('/content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/glove.6B.300d.txt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMK-MFfcHgt-"},"outputs":[],"source":["#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n","from numpy import asarray\n","\n","embeddings_index = dict()\n","for line in glove_vector:\n","\tvalues = line.split()\n","\tword = values[0]\n","\tcoefs = asarray(values[1:], dtype='float32')\n","\tembeddings_index[word] = coefs\n","glove_vector.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzSS38mlIJN1"},"outputs":[],"source":["#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n","# create a weight matrix for words in training docs\n","\n","from numpy import zeros\n","\n","embedding_matrix = zeros((vocab_size, 300))\n","for word, i in t.word_index.items():\n","\tembedding_vector = embeddings_index.get(word)\n","\tif embedding_vector is not None:\n","    \n","\t\tembedding_matrix[i] = embedding_vector\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dy_WLkne6VKb"},"source":["Getting Categorial Features ready for EMbedding layer"]},{"cell_type":"markdown","metadata":{"id":"79dUjcPI8n3F"},"source":["- School State"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcVEoCUS6a6G"},"outputs":[],"source":["# Using Tokenizer to encode Categorical features and then Post padding them\n","\n","# School State\n","#Appraoch - 1- Fitting the Tokenizer on the lists of categorical values\n"," \n","t2 = Tokenizer()\n","\n","t2.fit_on_texts(X_train[\"school_state\"])\n","\n","vocab_size_School_State = len(t2.word_index) + 1\n","\n","# integer encode the documents\n","encoded_school_state_train = t2.texts_to_sequences(X_train[\"school_state\"])\n","encoded_school_state_test=  t2.texts_to_sequences(X_test[\"school_state\"])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pj1I45sk7urJ"},"outputs":[],"source":["from keras.preprocessing import sequence\n","\n","#Maxlen is going to be 1 always as its categorical encoding\n","\n","padded_school_state_train = sequence.pad_sequences(encoded_school_state_train, maxlen = 1, padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jz_0UMdw8RDg"},"outputs":[],"source":["padded_school_state_test = sequence.pad_sequences(encoded_school_state_test, maxlen = 1, padding='post')"]},{"cell_type":"markdown","metadata":{"id":"dKt5kh5w8rYL"},"source":["- project grade category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SP40_2UG0sAf"},"outputs":[],"source":["t2 = Tokenizer()\n","\n","t2.fit_on_texts(X_train[\"project_grade_category\"])\n","\n","\n","vocab_size_project_project_grade = len(t2.word_index) + 1\n","\n","# integer encode the documents\n","encoded_project_grade_category_train = t2.texts_to_sequences(X_train[\"project_grade_category\"])\n","encoded_project_grade_category_test=  t2.texts_to_sequences(X_test[\"project_grade_category\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKBB9tEP9CMH"},"outputs":[],"source":["padded_project_grade_category_train = sequence.pad_sequences(encoded_project_grade_category_train, maxlen = 1, padding='post')\n","\n","padded_project_grade_category_test = sequence.pad_sequences(encoded_project_grade_category_test, maxlen = 1, padding='post')"]},{"cell_type":"markdown","metadata":{"id":"5g3Em9HL9d-c"},"source":["- input clean categories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1FDmymh9jba"},"outputs":[],"source":["t2 = Tokenizer()\n","\n","t2.fit_on_texts(X_train[\"clean_categories\"])\n","\n","\n","vocab_size_clean_categories = len(t2.word_index) + 1\n","\n","# integer encode the documents\n","encoded_clean_categories_train = t2.texts_to_sequences(X_train[\"clean_categories\"])\n","encoded_clean_categories_test=  t2.texts_to_sequences(X_test[\"clean_categories\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mqcZy_Y9kG_"},"outputs":[],"source":["padded_clean_categories_train = sequence.pad_sequences(encoded_clean_categories_train, maxlen = 1, padding='post')\n","\n","padded_clean_categories_test = sequence.pad_sequences(encoded_clean_categories_test, maxlen = 1, padding='post')"]},{"cell_type":"markdown","metadata":{"id":"MP5N0nP494a3"},"source":["- clean subcategories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9yzYqnK938x"},"outputs":[],"source":["t2 = Tokenizer()\n","\n","t2.fit_on_texts(X_train[\"clean_subcategories\"])\n","\n","\n","vocab_size_clean_subcategories = len(t2.word_index) + 1\n","\n","# integer encode the documents\n","encoded_clean_subcategories_train = t2.texts_to_sequences(X_train[\"clean_subcategories\"])\n","encoded_clean_subcategories_test=  t2.texts_to_sequences(X_test[\"clean_subcategories\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1sMD7lu-CKd"},"outputs":[],"source":["padded_clean_subcategories_train = sequence.pad_sequences(encoded_clean_subcategories_train, maxlen = 1, padding='post')\n","\n","padded_clean_subcategories_test = sequence.pad_sequences(encoded_clean_subcategories_test, maxlen = 1, padding='post')"]},{"cell_type":"markdown","metadata":{"id":"6uZimXxP-RiD"},"source":["- input teacher prefix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j03kFhiN-XpU"},"outputs":[],"source":["t2 = Tokenizer()\n","\n","t2.fit_on_texts(X_train[\"teacher_prefix\"])\n","\n","vocab_size_teacher_prefix = len(t2.word_index) + 1\n","\n","# integer encode the documents\n","encoded_teacher_prefix_train = t2.texts_to_sequences(X_train[\"teacher_prefix\"])\n","encoded_teacher_prefix_test=  t2.texts_to_sequences(X_test[\"teacher_prefix\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpvGeAWI-aZA"},"outputs":[],"source":["padded_teacher_prefix_train = sequence.pad_sequences(encoded_teacher_prefix_train, maxlen = 1, padding='post')\n","\n","padded_teacher_prefix_test = sequence.pad_sequences(encoded_teacher_prefix_test, maxlen = 1, padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103310,"status":"ok","timestamp":1619594941763,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"omjNju7DvwwY","outputId":"91966239-7fa5-40fd-ce05-ebf399725655"},"outputs":[{"data":{"text/plain":["array([1], dtype=int32)"]},"execution_count":33,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["padded_teacher_prefix_train[15660]"]},{"cell_type":"markdown","metadata":{"id":"SAd02gbaY-9p"},"source":["Model 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bg8L9OoWm5x9"},"outputs":[],"source":["# Defiing Custom AUC for keras\n","#https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n","\n","from sklearn.metrics import roc_auc_score\n","\n","# def auroc(y_true, y_pred):\n","#     score = tf.py_function( lambda y_true, y_pred : roc_auc_score( y_true, y_pred, average='macro', sample_weight=None).astype('float32'),\n","#                         [y_true, y_pred],\n","#                         'float32',\n","#                         stateful=True,\n","#                         name='sklearnAUC' )\n","#     return score\n","\n","def auroc(y_true, y_pred):\n","    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2206,"status":"ok","timestamp":1619599039370,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"32OpjPFpY9de","outputId":"ee37e7b1-bf4a-4916-c2c2-544dff7558fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 300)]        0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 300, 300)     15165600    input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 300, 300)     0           embedding[0][0]                  \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 300, 128)     219648      dropout[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 1, 26)        1352        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 1, 3)         18          input_3[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 1, 3)         30          input_4[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_4 (Embedding)         (None, 1, 3)         48          input_5[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, 1, 50)        1900        input_6[0][0]                    \n","__________________________________________________________________________________________________\n","input_7 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 38400)        0           lstm[0][0]                       \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 26)           0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 3)            0           embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","flatten_3 (Flatten)             (None, 3)            0           embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","flatten_4 (Flatten)             (None, 3)            0           embedding_4[0][0]                \n","__________________________________________________________________________________________________\n","flatten_5 (Flatten)             (None, 50)           0           embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 24)           48          input_7[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 38509)        0           flatten[0][0]                    \n","                                                                 flatten_1[0][0]                  \n","                                                                 flatten_2[0][0]                  \n","                                                                 flatten_3[0][0]                  \n","                                                                 flatten_4[0][0]                  \n","                                                                 flatten_5[0][0]                  \n","                                                                 dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128)          4929280     concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 64)           256         dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 32)           2080        batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 2)            66          dense_3[0][0]                    \n","==================================================================================================\n","Total params: 20,328,582\n","Trainable params: 5,162,854\n","Non-trainable params: 15,165,728\n","__________________________________________________________________________________________________\n","None\n"]}],"source":["\n","from keras.callbacks import ModelCheckpoint,EarlyStopping\n","from keras.layers import LSTM, Flatten, Dense,BatchNormalization\n","from keras.initializers import he_normal\n","from keras.optimizers import Adam\n","from keras.regularizers import l2\n","\n","from keras.callbacks import TensorBoard\n","from time import time\n","\n","#clearing the graph of tensorflow\n","tf.keras.backend.clear_session()\n","\n","#300 is the size of sequence 'cause of padding\n","input_1 = Input(shape=(300,))\n","\n","E_layer1 = Embedding(input_dim=vocab_size,output_dim= 300,weights=[embedding_matrix],trainable=False)(input_1)\n","E_layer1 = Dropout(0.3)(E_layer1)\n","\n","E_layer1 = LSTM(128,return_sequences=True)(E_layer1)\n","E_layer1 = Flatten()(E_layer1)\n","\n","#https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n","cat_vars = [\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"clean_categories\",\"clean_subcategories\"]\n","cat_sizes = {}\n","cat_embsizes = {}\n","for cat in cat_vars:\n","    cat_sizes[cat] = X_train[cat].nunique()\n","    cat_embsizes[cat] = min(50, cat_sizes[cat]//2+1)  \n","\n","\n","#Input State\n","input_2 = Input(shape=(1,))\n","E_layer2 = Embedding(input_dim=vocab_size_School_State, output_dim=cat_embsizes['school_state'])(input_2)\n","E_layer2 = Flatten()(E_layer2)\n","\n","#teacher_prefix\n","input_3 = Input(shape=(1,))\n","E_layer3 = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=cat_embsizes['teacher_prefix'])(input_3)\n","E_layer3 = Flatten()(E_layer3)\n","\n","# project _ grade _ category\n","input_4 = Input(shape=(1,))\n","E_layer4 = Embedding(input_dim=vocab_size_project_project_grade, output_dim=cat_embsizes['project_grade_category'])(input_4)\n","E_layer4 = Flatten()(E_layer4)\n","\n","#Clean Categories\n","input_5 = Input(shape=(1,))\n","E_layer5 = Embedding(input_dim=vocab_size_clean_categories, output_dim=cat_embsizes['project_grade_category'])(input_5)\n","E_layer5 = Flatten()(E_layer5)\n","\n","#Clean Subcategories\n","input_6 = Input(shape=(1,))\n","E_layer6 = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=cat_embsizes['clean_subcategories'])(input_6)\n","E_layer6 = Flatten()(E_layer6)\n","\n","#numerical features\n","input_7 = Input(shape=(1,))\n","Dense_numerical = Dense(24,activation='relu')(input_7)\n","\n","#Concatlayer\n","concat_layer = concatenate([E_layer1,E_layer2, E_layer3, E_layer4,E_layer5,E_layer6, Dense_numerical])\n","\n","# Dense Layer\n","Dense_concat1 = Dense(128, activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat_layer)\n","\n","#Drop\n","Drop_layer1 = Dropout(0.3)(Dense_concat1)\n","\n","#Dense Layer\n","Dense_concat2 = Dense(64, activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(Drop_layer1)\n","\n","#Drop\n","Drop_layer2 =Dropout(0.3)(Dense_concat2)\n","\n","Batch_Normal = BatchNormalization()(Drop_layer2)\n","\n","# Dense\n","Dense_concat3 = Dense(32, activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(Batch_Normal)\n","\n","#Dense Output\n","\n","Dense_output = Dense(2, activation = 'softmax')(Dense_concat3)\n","\n","model = Model([input_1, input_2, input_3, input_4, input_5, input_6, input_7], Dense_output)\n","\n","\n","# introducing checkpoints to save the best model weights\n","#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n","#https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n","\n","\n","filepath=\"content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\"\n","\n","checkpoints =  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","\n","tensorboard = TensorBoard(log_dir='/content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/logs/{}'.format(time()))\n","\n","\n","es = EarlyStopping(monitor='val_loss',patience=4, verbose=1)\n","\n","callback_list = [checkpoints, tensorboard,es]\n","\n","model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0006,decay = 1e-4), metrics=['accuracy', auroc])\n","print(model.summary())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhVRAukUesPl"},"outputs":[],"source":["# Changing the y into Categorical variable\n","\n","#https://stackoverflow.com/questions/61464888/tensorflow-error-valueerrorshapes-s-and-s-are-incompatible-self-other\n","\n","from keras.utils import to_categorical\n","\n","y_train = to_categorical(y_train)\n","\n","y_test = to_categorical(y_test)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKd8G-TJ8LID"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":690,"status":"ok","timestamp":1619509010371,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"8AZwL8sQ8KaO","outputId":"7c756d3a-86bd-4f17-be13-7c4a390bed3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(81936, 2)\n"]}],"source":["print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vf8wq-pRiymO"},"outputs":[],"source":["# Combining All the features to have train and test\n","\n","Train_data =  [padded_docs, padded_school_state_train,padded_teacher_prefix_train, padded_project_grade_category_train, padded_clean_categories_train,padded_clean_subcategories_train, X_train[\"remaining_numerical_features\"]]\n","Test_data = [padded_docs_test, padded_school_state_test,padded_teacher_prefix_test, padded_project_grade_category_test, padded_clean_categories_test, padded_clean_subcategories_test, X_test[\"remaining_numerical_features\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":692914,"status":"ok","timestamp":1619600870190,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"jdEBB1mCmYN4","outputId":"bd626e10-6aa8-4973-ddce-b61421681f85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","429/429 [==============================] - 54s 125ms/step - loss: 0.4355 - accuracy: 0.8473 - auroc: 0.6451 - val_loss: 0.4203 - val_accuracy: 0.8480 - val_auroc: 0.7056\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.84803, saving model to content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\n","Epoch 2/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.4140 - accuracy: 0.8473 - auroc: 0.6960 - val_loss: 0.4016 - val_accuracy: 0.8501 - val_auroc: 0.7308\n","\n","Epoch 00002: val_accuracy improved from 0.84803 to 0.85007, saving model to content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\n","Epoch 3/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.4019 - accuracy: 0.8498 - auroc: 0.7151 - val_loss: 0.4076 - val_accuracy: 0.8481 - val_auroc: 0.7378\n","\n","Epoch 00003: val_accuracy did not improve from 0.85007\n","Epoch 4/20\n","429/429 [==============================] - 53s 124ms/step - loss: 0.3965 - accuracy: 0.8492 - auroc: 0.7250 - val_loss: 0.3988 - val_accuracy: 0.8482 - val_auroc: 0.7437\n","\n","Epoch 00004: val_accuracy did not improve from 0.85007\n","Epoch 5/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.3919 - accuracy: 0.8507 - auroc: 0.7336 - val_loss: 0.3946 - val_accuracy: 0.8510 - val_auroc: 0.7502\n","\n","Epoch 00005: val_accuracy improved from 0.85007 to 0.85096, saving model to content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\n","Epoch 6/20\n","429/429 [==============================] - 53s 124ms/step - loss: 0.3888 - accuracy: 0.8525 - auroc: 0.7388 - val_loss: 0.3911 - val_accuracy: 0.8532 - val_auroc: 0.7533\n","\n","Epoch 00006: val_accuracy improved from 0.85096 to 0.85318, saving model to content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\n","Epoch 7/20\n","429/429 [==============================] - 53s 124ms/step - loss: 0.4016 - accuracy: 0.8514 - auroc: 0.7012 - val_loss: 0.3975 - val_accuracy: 0.8549 - val_auroc: 0.7511\n","\n","Epoch 00007: val_accuracy improved from 0.85318 to 0.85488, saving model to content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\n","Epoch 8/20\n","429/429 [==============================] - 53s 124ms/step - loss: 0.3839 - accuracy: 0.8538 - auroc: 0.7485 - val_loss: 0.3880 - val_accuracy: 0.8530 - val_auroc: 0.7578\n","\n","Epoch 00008: val_accuracy did not improve from 0.85488\n","Epoch 9/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.3794 - accuracy: 0.8541 - auroc: 0.7597 - val_loss: 0.3808 - val_accuracy: 0.8525 - val_auroc: 0.7603\n","\n","Epoch 00009: val_accuracy did not improve from 0.85488\n","Epoch 10/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.3770 - accuracy: 0.8550 - auroc: 0.7620 - val_loss: 0.3962 - val_accuracy: 0.8494 - val_auroc: 0.7568\n","\n","Epoch 00010: val_accuracy did not improve from 0.85488\n","Epoch 11/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.3734 - accuracy: 0.8545 - auroc: 0.7675 - val_loss: 0.3826 - val_accuracy: 0.8552 - val_auroc: 0.7587\n","\n","Epoch 00011: val_accuracy improved from 0.85488 to 0.85521, saving model to content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\n","Epoch 12/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.3704 - accuracy: 0.8570 - auroc: 0.7727 - val_loss: 0.3867 - val_accuracy: 0.8537 - val_auroc: 0.7545\n","\n","Epoch 00012: val_accuracy did not improve from 0.85521\n","Epoch 13/20\n","429/429 [==============================] - 53s 123ms/step - loss: 0.3664 - accuracy: 0.8594 - auroc: 0.7796 - val_loss: 0.3930 - val_accuracy: 0.8491 - val_auroc: 0.7593\n","\n","Epoch 00013: val_accuracy did not improve from 0.85521\n","Epoch 00013: early stopping\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f40fe24ebd0>"]},"execution_count":50,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.fit(Train_data,y_train,validation_split=0.33,epochs=20,verbose=1,batch_size=128, callbacks =callback_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBEuSUH5dw6_"},"outputs":[],"source":["# Loading the model with the best weights learned through callbacks\n","\n","\n","from keras.callbacks import ModelCheckpoint,EarlyStopping\n","from keras.layers import LSTM, Flatten, Dense\n","from keras.initializers import he_normal\n","from keras.optimizers import Adam\n","\n","from keras.callbacks import TensorBoard\n","from time import time\n","\n","#clearing the graph of tensorflow\n","tf.keras.backend.clear_session()\n","\n","#300 is the size of sequence 'cause of padding\n","input_1 = Input(shape=(300,))\n","\n","E_layer1 = Embedding(input_dim=vocab_size,output_dim= 300,weights=[embedding_matrix],trainable=False)(input_1)\n","E_layer1 = Dropout(0.3)(E_layer1)\n","\n","E_layer1 = LSTM(128,return_sequences=True)(E_layer1)\n","E_layer1 = Flatten()(E_layer1)\n","\n","#https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n","cat_vars = [\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"clean_categories\",\"clean_subcategories\"]\n","cat_sizes = {}\n","cat_embsizes = {}\n","for cat in cat_vars:\n","    cat_sizes[cat] = X_train[cat].nunique()\n","    cat_embsizes[cat] = min(50, cat_sizes[cat]//2+1)  \n","\n","\n","#Input State\n","input_2 = Input(shape=(1,))\n","E_layer2 = Embedding(input_dim=vocab_size_School_State, output_dim=cat_embsizes['school_state'])(input_2)\n","E_layer2 = Flatten()(E_layer2)\n","\n","#teacher_prefix\n","input_3 = Input(shape=(1,))\n","E_layer3 = Embedding(input_dim=vocab_size_teacher_prefix, output_dim=cat_embsizes['teacher_prefix'])(input_3)\n","E_layer3 = Flatten()(E_layer3)\n","\n","# project _ grade _ category\n","input_4 = Input(shape=(1,))\n","E_layer4 = Embedding(input_dim=vocab_size_project_project_grade, output_dim=cat_embsizes['project_grade_category'])(input_4)\n","E_layer4 = Flatten()(E_layer4)\n","\n","#Clean Categories\n","input_5 = Input(shape=(1,))\n","E_layer5 = Embedding(input_dim=vocab_size_clean_categories, output_dim=cat_embsizes['project_grade_category'])(input_5)\n","E_layer5 = Flatten()(E_layer5)\n","\n","#Clean Subcategories\n","input_6 = Input(shape=(1,))\n","E_layer6 = Embedding(input_dim=vocab_size_clean_subcategories, output_dim=cat_embsizes['clean_subcategories'])(input_6)\n","E_layer6 = Flatten()(E_layer6)\n","\n","#numerical features\n","input_7 = Input(shape=(1,))\n","Dense_numerical = Dense(24,activation='relu')(input_7)\n","\n","#Concatlayer\n","concat_layer = concatenate([E_layer1,E_layer2, E_layer3, E_layer4,E_layer5,E_layer6, Dense_numerical])\n","\n","# Dense Layer\n","Dense_concat1 = Dense(128, activation='relu',kernel_initializer=he_normal())(concat_layer)\n","\n","#Drop\n","Drop_layer1 = Dropout(0.3)(Dense_concat1)\n","\n","#Dense Layer\n","Dense_concat2 = Dense(64, activation='relu',kernel_initializer=he_normal())(Drop_layer1)\n","\n","#Drop\n","Drop_layer2 =Dropout(0.3)(Dense_concat2)\n","\n","Batch_Normal = BatchNormalization()(Drop_layer2)\n","# Dense\n","Dense_concat3 = Dense(32, activation='relu',kernel_initializer=he_normal())(Batch_Normal)\n","\n","#Dense Output\n","\n","Dense_output = Dense(2, activation = 'softmax')(Dense_concat3)\n","\n","model = Model([input_1, input_2, input_3, input_4, input_5, input_6, input_7], Dense_output)\n","\n","\n","\n","filepath=\"content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/weights.best.hdf5\"\n","\n","tensorboard = TensorBoard(log_dir='/content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/logs/{}'.format(time()))\n","\n","model.load_weights(filepath)\n","\n","callback_list = [tensorboard]\n","\n","model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0006,decay = 1e-4), metrics=['accuracy', auroc])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66911,"status":"ok","timestamp":1619601123183,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"hjdZ0ZgvffBD","outputId":"d2cd4f9b-8856-4e1f-f6fa-0e8f7ecfa5b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train AUC: 0.7768090115041786\n"]}],"source":["# Prediction on Test Data\n","from sklearn.metrics import roc_auc_score\n","\n","# Train\n","y_train_pred = model.predict(Train_data)\n","print(\"Train AUC:\",roc_auc_score(y_train,y_train_pred))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22053,"status":"ok","timestamp":1619601164039,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"z39F7R4GgEDA","outputId":"85490a85-824c-4773-b934-25b41f59ca8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test AUC: 0.7211552553198166\n"]}],"source":["#Test\n","y_test_pred = model.predict(Test_data)\n","print(\"Test AUC:\",roc_auc_score(y_test,y_test_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dv3lKPh200VP"},"outputs":[],"source":["%reload_ext tensorboard\n","\n","%tensorboard --logdir=\"/content/drive/MyDrive/kaggle_folder/Donors Choose LSTM/Output/logs/\""]},{"cell_type":"markdown","metadata":{"id":"_VQWvV2zSCKd"},"source":["2. Model Two"]},{"cell_type":"markdown","metadata":{"id":"vAbpZq30AG8i"},"source":["Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below.\n","\n","1. Train the TF-IDF on the Train data \n","\n","2. Get the idf value for each word we have in the train data. \n","\n","3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)\n","\n","4. Train the LSTM after removing the Low and High idf value words. (In model-1 Train on total data but in Model-2 train on data after removing some words based on IDF values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ELS03F43irF"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(min_df=10)\n","tfidf = vectorizer.fit(X_train[\"essay\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgpAYJ7_qR3f"},"outputs":[],"source":["# Getting the idf values\n","idf = tfidf.idf_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2031,"status":"ok","timestamp":1620293184795,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"TWmOv4qs_xUm","outputId":"ccacb017-ad5d-4bf8-daf4-71dd8f130e97"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'>\n"]}],"source":["print(type(idf))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"elapsed":1141,"status":"ok","timestamp":1620293748763,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"l6ALA1ZiABHO","outputId":"65d0b2ac-e2f9-411d-c5e3-10932a33518c"},"outputs":[{"data":{"text/plain":["{'boxes': [<matplotlib.lines.Line2D at 0x7faada4cbe10>],\n"," 'caps': [<matplotlib.lines.Line2D at 0x7faada380ed0>,\n","  <matplotlib.lines.Line2D at 0x7faada389450>],\n"," 'fliers': [<matplotlib.lines.Line2D at 0x7faada389f10>],\n"," 'means': [],\n"," 'medians': [<matplotlib.lines.Line2D at 0x7faada3899d0>],\n"," 'whiskers': [<matplotlib.lines.Line2D at 0x7faada380450>,\n","  <matplotlib.lines.Line2D at 0x7faada380990>]}"]},"execution_count":29,"metadata":{"tags":[]},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAKcUlEQVR4nO3dQYyc91nH8d8Tex3bu7TYyioqNsI9oFy4FO0BaMShKVYFiHLgUEdFNa6U21IQEiqnlBuyEMInJKuYVKIOh1AJxAFRFaoqUhVpnVYiTZAqUVpi0mSbWDRCUew1DwdvSrxN7PHOZMd/z+cjrWb23dl5n8Pqq1f/fd93qrsDwHjum/cAAOyOgAMMSsABBiXgAIMScIBB7d/LnT3wwAN94sSJvdwlwPAuXbr0g+5e3bl9TwN+4sSJbGxs7OUuAYZXVd99p+2WUAAGJeAAgxJwgEHdNuBVdaGqXqmq59627WhVfbmqvr39eOS9HROAnSY5An8iycd2bPtskq90988m+cr29wDsodsGvLu/luS1HZs/nuQL28+/kOQ3ZzwXALex2zXwB7v7pe3n30/y4Lu9sKoeq6qNqtrY3Nzc5e4A2Gnqf2L2jfvRvus9abv7fHevdffa6uqPnYcOwC7t9kKel6vqA939UlV9IMkrsxwKplVVe7If99NnnnZ7BP73ST61/fxTSf5uNuPAbHT3HX3t5nfEm3mb5DTCJ5N8PclDVfViVX06yZ8k+ZWq+naSj25/D8Aeuu0SSnefepcfPTLjWQC4A67EBBiUgAMMSsABBiXgAIMScIBBCTjAoAQcYFACDjAoAQcYlIADDGq3dyOEPXP06NFcuXLlPd/Pe30HwyNHjuS113Z+NgrsnoBz17ty5co9cee/vbrFLYvDEgrAoAQcYFACDjAoAQcYlIADDErAAQYl4ACDEnCAQQk4wKAEHGBQAg4wKAEHGJSAAwxKwAEGJeAAgxJwgEEJOMCgBBxgUAIOMCificldrx9/X/K59897jKn14++b9wjcYwScu1798Q/vmQ817s/NewruJZZQAAYl4ACDEnCAQU0V8Kr6/ar6VlU9V1VPVtXBWQ0GwK3tOuBVdSzJ7yZZ6+6fS7IvySdmNRgAtzbtEsr+JIeqan+Sw0n+a/qRAJjErgPe3ZeT/GmS7yV5Kcl/d/c/7XxdVT1WVRtVtbG5ubn7SQG4yTRLKEeSfDzJB5P8VJLlqvrkztd19/nuXuvutdXV1d1PCsBNpllC+WiS73T3ZndfS/KlJL80m7EAuJ1pAv69JL9QVYerqpI8kuSF2YwFwO1Mswb+TJKnkjyb5F+33+v8jOYC4DamuhdKdz+e5PEZzQLAHXAlJsCgBBxgUAIOMCgBBxiUgAMMSsABBiXgAIMScIBBCTjAoAQcYFACDjAoAQcYlIADDErAAQYl4ACDEnCAQU31gQ6wV258at/Yjhw5Mu8RuMcIOHe97n7P91FVe7IfmCVLKACDEnCAQQk4wKAEHGBQAg4wKAEHGJSAAwxKwAEGJeAAgxJwgEEJOMCgBBxgUAIOMCgBBxiUgAMMSsABBiXgAIOaKuBV9ZNV9VRV/VtVvVBVvzirwQC4tWk/Uu1ckn/s7t+qqgNJDs9gJgAmsOuAV9X7k/xyktNJ0t1Xk1ydzVgA3M40SygfTLKZ5K+q6htV9fmqWt75oqp6rKo2qmpjc3Nzit0B8HbTBHx/kp9P8hfd/aEk/5Pksztf1N3nu3utu9dWV1en2B0AbzdNwF9M8mJ3P7P9/VO5EXQA9sCuA97d30/yn1X10PamR5I8P5OpALitac9CWU/yxe0zUP49ye9MPxIAk5gq4N39zSRrM5oFgDvgSkyAQQk4wKAEHGBQAg4wKAEHGJSAAwxKwAEGJeAAgxJwgEEJOMCgBBxgUAIOMCgBBxiUgAMMSsABBiXgAIMScIBBCTjAoAQcYFACDjAoAQcYlIADDErAAQYl4ACDEnCAQQk4wKAEHGBQAg4wKAEHGJSAAwxKwAEGJeAAgxJwgEEJOMCgBBxgUFMHvKr2VdU3quofZjEQAJOZxRH4Z5K8MIP3AeAOTBXwqjqe5NeSfH424wAwqWmPwP88yR8m+d8ZzALAHdh1wKvq15O80t2XbvO6x6pqo6o2Njc3d7s7AHaY5gj8w0l+o6r+I8nfJPlIVf31zhd19/nuXuvutdXV1Sl2B8Db7Trg3f1H3X28u08k+USSf+7uT85sMgBuyXngAIPaP4s36e6vJvnqLN4LgMk4AgcYlIADDErAAQYl4Cy0paWlVFWSpKqytLQ054lgcgLOwlpaWsrW1tZN27a2tkScYQg4C2tnvG+3He42MzmNEO42by2LvNe/391T7QemIeDckyYJ660iLcyMwBIKwKAEHGBQAg4wKAEHGJSAAwxKwAEGJeAAgxJwgEEJOMCgBBxgUAIOMCgBBxiUgAMMSsABBiXgAIMScIBBCTjAoAQcYFACDjAoAQcYlIADDErAAQYl4ACDEnCAQQk4wKAEHGBQAg4wKAEHGNSuA15VP11V/1JVz1fVt6rqM7McDIBb2z/F724l+YPufraqfiLJpar6cnc/P6PZALiFXR+Bd/dL3f3s9vPXk7yQ5NisBgPg1mayBl5VJ5J8KMkz7/Czx6pqo6o2Njc3Z7E7ADKDgFfVSpK/TfJ73f3DnT/v7vPdvdbda6urq9PuDoBtUwW8qpZyI95f7O4vzWYkACYxzVkoleQvk7zQ3X82u5EAmMQ0R+AfTvLbST5SVd/c/vrVGc0Fe+bGscj/P8Iodn0aYXc/ncRfPMNbWlrKtWvXsrS0lKtXr857HJjYNOeBwz3hrWiLN6NxKT0L78EHH7zpEUYh4Cyst9a8X3755ZserYUzCgFnYXX3HW2Hu42As/Duu+++mx5hFP5iWXirq6upqrhSmNEIOAvtwIEDOXToUKoqhw4dyoEDB+Y9EkxMwFlo165dy/r6el5//fWsr6/n2rVr8x4JJlZ7+Q+btbW13tjY2LP9wa3c6mwT/8jkblJVl7p7bed2R+AsrKNHj97RdrjbuBKThXX48OG88cYb2dra+tGl9Pv378/hw4fnPRpMxBE4C+vy5ctZWVnJsWPHUlU5duxYVlZWcvny5XmPBhMRcBbWgQMHcvLkySwvL6eqsry8nJMnTzoThWFYQmFhXb16NRcvXvzRPVBeffXVXLx4cc5TweQcgbOw9u3bl+Xl5Rw8eDDdnYMHD2Z5eTn79u2b92gwEQFnYW1tbWVlZSUXLlzIm2++mQsXLmRlZSVbW1vzHg0mIuAstNOnT2d9fT0HDx7M+vp6Tp8+Pe+RYGLWwFlYx48fzxNPPJGLFy/m4YcfztNPP51HH300x48fn/doMBFH4Cyss2fP5vr16zlz5kzuv//+nDlzJtevX8/Zs2fnPRpMRMBZWKdOncq5c+duOo3w3LlzOXXq1LxHg4m4FwrAXc69UADuMQIOMCgBBxiUgAMMSsABBrWnZ6FU1WaS7+7ZDmFyDyT5wbyHgHfxM939Y5+6vacBh7tVVW2802lacDezhAIwKAEHGJSAww3n5z0A3Clr4ACDcgQOMCgBBxiUgLPQqupCVb1SVc/Nexa4UwLOonsiycfmPQTshoCz0Lr7a0lem/ccsBsCDjAoAQcYlIADDErAAQYl4Cy0qnoyydeTPFRVL1bVp+c9E0zKpfQAg3IEDjAoAQcYlIADDErAAQYl4ACDEnCAQQk4wKD+D9L3VTVX2pOiAAAAAElFTkSuQmCC","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light","tags":[]},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","plt.boxplot(idf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leh9RrrECXsb"},"outputs":[],"source":["percentile_25th  = np.percentile(idf,25)\n","percentile_75th  = np.percentile(idf,75)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1279,"status":"ok","timestamp":1620294696773,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"8DLJkUrjFh7p","outputId":"a07832da-2539-4192-d26d-8e2e15165f04"},"outputs":[{"name":"stdout","output_type":"stream","text":["25 the percentile is  6.990695959136977\n","75 the percentile is 9.317973664721395\n"]}],"source":["print(\"25 the percentile is\", percentile_25th )\n","print(\"75 the percentile is\", percentile_75th)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1229,"status":"ok","timestamp":1620294744464,"user":{"displayName":"Nikhil Jaiswal","photoUrl":"","userId":"14188135524867243014"},"user_tz":-330},"id":"k6TC_bnjF1Vf","outputId":"9127484a-cfa0-4a4e-dcbf-fc6a466bb646"},"outputs":[{"name":"stdout","output_type":"stream","text":["14898\n","14898\n"]}],"source":["print(len(vectorizer.get_feature_names()))\n","print(len(idf))\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Copy of LSTM - Assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}
